# Transformation

The **Transformation** layer in the Data BackBone (DBB) platform is responsible for modeling, validating, and publishing curated data products.  
Transformations are metadata-driven, fully automated, and executed through Microsoft Fabric pipelines and dbt.  

This section provides an overview of the key Fabric artefacts and dbt project components that make up the transformation workload.

---

## Fabric Artefacts

| Artefact | Git Location | Purpose |
|-----------|---------------|----------|
| **`transformation_pipeline_DBB`** | `fabric/transformation/transformation_pipeline_DBB.DataPipeline/pipeline-content.json` | Generic Fabric pipeline that executes dbt models for any Data Product. It retrieves environment configuration, runs the `transformation_notebook`, performs validation and metadata refresh, and handles failure logic. |
| **`transformation_notebook`** | `fabric/transformation/transformation_notebook.Notebook/notebook-content.py` | Generic notebook that runs dbt builds per Data Product, validates that required integration tables have successfully loaded, and logs results into `dbb_warehouse.meta.monitoring_transformation_dbt`. |

> Each Data Product orchestration invokes this pipeline as part of its transformation stage.  
> Dependencies on completed integration runs are validated directly inside the `transformation_notebook`.  
> See [transformation_pipeline_<Data Product>](/Data-BackBone-Fabric-Implementation-Documentation/20.-Transformation/transformation_pipeline_DBB) and [transformation_notebook](/Data-BackBone-Fabric-Implementation-Documentation/20.-Transformation/transformation_notebook) for details.

---

## dbt Project Structure

| Component | Path | Description |
|------------|------|-------------|
| **Project Configuration** | `transformation/dbt/dbt_project.yml` | Defines schemas, materializations, model naming conventions, and environment targets. |
| **Profiles** | `transformation/dbt/profiles.yml` | Contains environment-specific connection settings for `tst`, `acc`, `qual`, and `prd`. |
| **Models** | `transformation/dbt/models` | Houses all dbt models organized by layer:<br>• **Silver (`sv_`)** – standardized and cleaned sources.<br>• **Raw Vault (`rv_`)** – historized and hashed data.<br>• **Business Vault** – business logic and transformations.<br>• **Gold (`dds_`)** – analytical and reporting-ready data. |
| **Macros** | `transformation/dbt/macros` | Reusable SQL and Jinja logic (e.g., hash key generation, SCD handling, date logic). |
| **Seeds** | `transformation/dbt/seeds` | Static reference data automatically loaded into the warehouse during dbt runs. |

---

## Operational Guidance

- **Version Control:**  
  Always export updated notebooks after changes and commit the latest `notebook-content.py` to Git.

- **Testing and Promotion:**  
  Run `dbt build` in the target environment (`tst`, `acc`, `qual`, or `prd`) and validate the results before promoting.  
  Store detailed logs in `logs/dbt.log` when debugging or auditing.

- **Pipeline Maintenance:**  
  When new notebooks, dbt models, or data products are introduced, ensure the `transformation_pipeline_DBB` includes the correct parameters and tasks.  
  Format and review all pipeline JSONs before committing to maintain readability.

- **Monitoring:**  
  Transformation results and dbt metrics are stored in `dbb_warehouse.meta.monitoring_transformation_dbt` for traceability and performance analysis.  
  (See [Monitoring Documentation](monitoring_overview) for details.)

---

## 📄 Related Documentation
[[_TOSP_]]
