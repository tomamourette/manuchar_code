# Integration

The **Integration** layer in the Data BackBone (DBB) platform is responsible for bringing data from various operational and external systems into the platform in a standardized, governed, and traceable way.  
Integration pipelines leverage **metadata-driven design** and **parameterized configuration** to ensure ingestion is consistent, environment-agnostic, and easy to maintain.

All ingested data lands in the **dbb_lakehouse**, which serves as the single integration layer across all Data Products.  
From there, data becomes available for transformation, validation, and publication in downstream workloads.

---

## Integration Architecture

The integration workload in Microsoft Fabric is implemented using two coordinated pipeline levels:

| Level | Pipeline | Description |
|--------|-----------|-------------|
| **Data Product Integration** | [`integration_pipeline_<Data Product>`](/Data-BackBone-Fabric-Implementation-Documentation/10.-Integration/integration_pipeline_<Data-Product>) | The parent pipeline for each Data Product. Resolves environment configuration, orchestrates all required source-system integrations, and handles metadata refresh and monitoring. |
| **Source-Specific Integration** | [`integration_pipeline_source_<Source System>`](/Data-BackBone-Fabric-Implementation-Documentation/10.-Integration/integration_pipeline_source_<Source-System>) | Dedicated pipeline for ingesting data from a single physical source system. Uses metadata to identify tables, define load types, and construct dynamic queries. |

This two-tier approach ensures modularity:
- Each Data Product can include multiple source systems.  
- Each source system can be reused across different Data Products.  
- Environment-specific configuration is dynamically resolved from metadata.

---

## Metadata Management

All configuration for integration pipelines is **version-controlled** within the DBB Azure DevOps repository.  
Metadata defines what to ingest, how to load it, and where to land it.

| Metadata File | Purpose |
|----------------|----------|
| `integration/integration_source_connections.csv` | Defines source and monitoring connection IDs per environment. |
| `integration/<source_name>/integration_source_list.csv` | Lists datasets to ingest for a given source. |
| `integration/<source_name>/<schema>/<table>.json` | Defines table-level ingestion metadata such as query, load type, and update field. |

Changes to metadata are made through version-controlled commits, ensuring full auditability and consistent synchronization between DevOps and Fabric.

---

## Incremental and Full Loading

Each dataset can be configured for:
- **Full Load** â€“ Reloads the entire dataset from the source.  
- **Incremental Load** â€“ Loads only new or updated records since the last successful ingestion.  

Incremental logic is handled dynamically at runtime:
- The **latest successful watermark** is retrieved from `dbb_warehouse.meta.monitoring_integration`.  
- The query is adjusted automatically to include only rows where the update field is greater than that watermark.  
- Full loads default to a placeholder watermark (`0000-00-00 00:00:00`).

This metadata-driven approach ensures consistency and minimizes duplication across all sources.

---

## Monitoring and Traceability

After every ingestion, metrics are written to `dbb_warehouse.meta.monitoring_integration`.  
This includes:
- Source, schema, and table names  
- Load type, row counts, and execution duration  
- Timestamps and status (Success/Failure)  

These logs provide **end-to-end observability** and feed into the DBB monitoring framework for validation and alerting.  
(See [Monitoring Documentation](<link to monitoring documentation page>) for more details.)

---

## Environment Management

Integration pipelines are **fully parameterized** and designed for cross-environment execution.  
Environment-specific details (workspace ID, connections, Lakehouse IDs, etc.) are dynamically resolved from:
- `dbb_warehouse.meta.environments` table  
- `integration_source_connections.csv` metadata  

This allows the same pipelines to be deployed seamlessly across **TST**, **ACC**, **QUAL**, and **PRD** workspaces without modification.

---

## Summary

The DBB Integration framework provides:
- A **centralized, metadata-driven** ingestion model  
- **Reusable pipelines** for Data Products and Sources  
- **Full auditability and monitoring** across environments  
- **Dynamic configuration** that supports scalable, low-maintenance operation  

ðŸ“„ **Related Documentation:**
[[_TOSP_]]