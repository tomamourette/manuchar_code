# Orchestration and Core Components

This page maps the orchestration and component assets stored in the repository to their role and purpose within the Microsoft Fabric environment for the Data BackBone (DBB) platform.

---

## Fabric Items

| Item | Git Location | Description |
|------|---------------|--------------|
| **orchestration_pipeline_<Data Product>** | `fabric/orchestration/orchestration_pipeline_<Data Product>.DataPipeline/pipeline-content.json` | Defines the end-to-end orchestration for each Data Product. Coordinates ingestion, transformation, reconciliation, and semantic model refresh in sequence. |
| **integration_pipeline_<Data Product>** | `fabric/integration/integration_pipeline_<Data Product>.DataPipeline/pipeline-content.json` | Handles ingestion of mirrored or raw source data for the specific Data Product into `dbb_lakehouse`. |
| **transformation_pipeline_DBB** | `fabric/transformation/transformation_pipeline_DBB.DataPipeline/pipeline-content.json` | Generic transformation pipeline that executes dbt models for any Data Product, using parameters passed from the orchestration pipeline. |
| **reconciliation_pipeline_DBB** | `fabric/reconciliation/reconciliation_pipeline_DBB.DataPipeline/pipeline-content.json` | Generic pipeline that runs reconciliation and quality checks on DBB models before refresh. |
| **data_product_refresh_DBB** | `fabric/transformation/data_product_refresh_DBB.DataPipeline/pipeline-content.json` | Generic pipeline that refreshes semantic models for a Data Product after successful reconciliation. |
| **dbb_lakehouse** | `fabric/dbb_lakehouse.Lakehouse` | Central Lakehouse storing mirrored operational source data (MDS, Dynamics, Mona, STG, generic datasets, etc.). |
| **dbb_warehouse** | `fabric/dbb_warehouse.Warehouse` | SQL analytics endpoint used by dbt for transformations and by Power BI for reporting. |

---

## Deployment Notes

* Use `cicd/deploy_workspace_items.py` with the appropriate environment variables (`FABRIC_WORKSPACE_ID`, `FABRIC_TARGET_ENV`) to republish the Fabric assets.  
* Replace environment-specific parameters in `fabric/parameter.yml` before deployment.  
* When updating a Fabric pipeline, **export the JSON definition from Fabric** (“Download definition”) and commit it to Git to keep Fabric and the repository in sync.  
* Each Data Product’s orchestration pipeline should be referenced under `fabric/orchestration/` with consistent naming and structure.

---

## Operational Tips

* Monitor pipeline runs in the Fabric portal under **Run History** to identify failed or delayed activities.  
* Cross-reference failed runs with DBB monitoring notebooks or logs for deeper diagnostics.  
* When introducing new components or data products, add them to this table and include them in the relevant orchestration pipeline for proper sequencing.  
* Ensure that concurrency settings and schedules are aligned with the requirements defined per Data Product.  

---

📄 **Related Documentation:**  
[[_TOSP_]]
