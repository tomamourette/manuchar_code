
Overview & Capabilities
-----------------------

The Integration layer's main capability is to bring source data into the Data Platform in a structured way. The integration methods vary based on the source location and type, but all operate within a decoupled system that remains agnostic of the technical workings of the source system. The Integration layer provides the following capabilities:
*   Data arrives in the Lakehouse of the Data BackBone as Delta Parquet files in the correct location.
    *   The following naming convention will be used: `<source name>__<schema name>_<table name>`.
    *   Data should be stored in tables under the `dbo` schema of the Lakehouse to increase consistency.
    *   All these parameters of the naming conventions should be stored in the source-controlled Integration metadata in Azure DevOps.
*   Ingestion can occur in batch, micro-batch, or streaming mode, ultimately appending data to the Delta files to maintain a history of ingested data.
*   The incoming data will be stored in the Lakehouse in the bronze layer as-is, meaning no transformations are applied at this stage. This approach ensures:
    *   Full traceability and auditability of ingested data.
    *   Improved consistency between layers.
    *   Isolation of transformations and business logic in later stages.
*   A unique combination of metadata is always present in the Lakehouse location structure. This metadata includes, at a minimum:
    *   A reference to the source system, schema, and table of the ingested data.
    *   Source information and ingestion timestamps for clarity and traceability.

Integration is executed through predefined Integration Flows, each designed to meet different integration requirements:
*   A fully managed integration flow within Microsoft Fabric’s Data Pipelines for sources that require only batch ingestion or cannot be integrated using other mechanisms like CDC tooling.
*   Change Data Capture (CDC) tooling, specifically Qlik Replicate, to support incremental, event-driven ingestion for near-real-time data integration.
*   Additional integration methods may be introduced as needed throughout the project's lifespan. For example, an IoT-based setup where different sensors or devices send data to an event hub, consolidating information into the data platform for near-real-time insights.

* * *

Integration Flows
-----------------

### Integration Flow: Fabric Data Pipelines

Data Pipelines in Microsoft Fabric are designed for batch loads and operate on a pull-based integration system.
The Data BackBone’s integration pipelines are structured using a generic metadata-driven ingestion framework. This framework focuses on the physical and logical separation of infrastructure and ingestion content, ensuring:
*   Infrastructure components remain independent from the ingestion logic.
*   Pipelines can be flexibly adapted without dependencies on the underlying infrastructure.
*   A consistent approach to ingestion is maintained across the platform.
This decoupling principle is applied throughout the entire Data BackBone, ensuring seamless integration without creating dependencies between the infrastructure used and the resulting content of the pipelines.

### Integration Flow: Change Data Capture (CDC) Tooling

Change Data Capture (CDC) tools enable event-driven and incremental data ingestion. Unlike batch pipelines, CDC follows a push-based integration system that provides near-real-time data availability.
In this project, Qlik Replicate has been chosen as the CDC tool. It works by:
*   Monitoring changes in the source system (e.g., inserts, updates, deletes).
*   Capturing these changes in real time.
*   Pushing them into the Data Platform as structured Delta Parquet files.
This approach facilitates near-real-time reporting and analytics by ensuring that the most up-to-date source data is available in the Data BackBone without requiring full reloads.

* * *

Integration Data Sources
------------------------

Each source system must be mapped to the environments in scope for the Manuchar Data BackBone. These environments are:
*   **DEV (Development):** Typically used by IT for development purposes. However, since we are using a SaaS approach, we will not be utilizing this environment. This makes sense because SaaS platforms are designed to provide managed infrastructure, reducing the need for a dedicated development environment.
*   **TST (Test):** This is where new development within the data platform in Microsoft Fabric will occur. It will serve as the primary environment for validating integration setups before moving to acceptance.
*   **ACC (Acceptance):** This environment is designated for business-facing user testing. It contains controlled datasets to allow validation and feedback from end users without impacting production data.
*   **PRD (Production):** The production environment is an isolated space containing all validated and deployed data resources. These resources are maintained and updated according to defined schedules to ensure consistency and reliability.

To ensure correct information reflection, maintain data security, enforce access privileges, and handle data sensitivity properly, it is essential to correctly map data sources to these environments. More details on how different data sources are mapped can be found in the following SharePoint document:
[Manuchar Data BackBone - Data Source Overview]([Overview DBB source connections.xlsx](https://manuchar.sharepoint.com/:x:/s/DataBackbone/EVi2HlYQrklFppr-U73CtogB9woBl6r6076ISI2qFo7nQQ?e=nOrlfR))

* * *

Integration Example
-------------------

Regardless of the Integration Flow used, data will always arrive **as-is** in the Lakehouse, only enriched with technical metadata. Any further transformations or enrichments should be handled in subsequent layers and should be first ingested as a separate dataset.

**Example: CustTrans Table from Dynamics**
When the `CustTrans` table from **Dynamics** is ingested into the Data BackBone, it follows the predefined naming convention:
*   **Naming Convention:** `<source name>__<schema name>_<table name>`
*   **Physical Representation in the Lakehouse:** `Dynamics__dbo_CustTrans`
*   **Schema:** The data will be stored under the `dbo` schema of the Lakehouse.
The metadata included with this dataset will contain:
*   **Source Name:** Aligned with the sources defined in the Integration metadata.
*   **Integration Timestamp:** Captures when the as-is data was received by the Data Platform, ensuring traceability.

The next step for this dataset is its reference in the **silver layer source views**, where it will be further enriched and transformed according to business requirements.