# Transformation solution design
The transformation layer combines Fabric, dbt and notebook patterns to convert raw integration data into curated outputs.

## Objectives
- Apply Data Vault 2.0 principles in the silver layer to retain auditability and enable agile change.
- Deliver dimensional models in the gold layer optimised for financial reporting and analytics.
- Centralise business logic in reusable dbt macros and models while supporting complex scenarios with notebooks when required.

## Logical layers
1. **Raw Vault (mim schema):** Hubs, links and satellites generated from integration feeds. Follow the standards in [[Data-BackBone-Solution-Design-Documentation/Silver-%2D-Data-Vault-2.0-modeling-best-practices.md]].
2. **Business Vault (mim schema):** Derivations, calculations and data quality enhancements stored as views. Leverage reusable macros for hash keys and change detection.
3. **Operational Data Store (ods schema):** Flattened staging views targeted at downstream logical models.
4. **Dimensional layer (dds_finance schema):** Stars, aggregates and KPIs ready for semantic models; see [[Data-BackBone-Solution-Design-Documentation/Gold-%2D-Dimensional-model-best-practices.md]].

## Tooling standards
- Use dbt as the primary transformation engine (`transformation/dbt`). Each model must declare tests (not null, unique) and refer to documented sources.
- Macros encapsulate repeatable patterns such as hash key generation, SCD handling and calendar logic (`transformation/dbt/macros`).
- For heavyweight transformations or external dependencies, implement Fabric notebooks in `fabric/transformation` and orchestrate them via `transformation_pipeline_DBB`.

## Development guidelines
- Model naming: prefix silver layer views with `sv_`, raw vault tables with `rv_`, gold tables with `gold_` or domain specific prefixes.
- Keep business rules in source controlled code. Document decisions alongside models using dbt docs blocks.
- Enforce idempotent runs: dbt models should succeed when executed multiple times; notebooks must handle restart scenarios gracefully.

## Performance considerations
- Partition large tables on natural business keys (for example period or company) and ensure indexes are defined in the warehouse where supported.
- Use incremental models in dbt when the source supplies change timestamps; fall back to full refresh with truncation when required.
- Monitor run durations via the Fabric monitoring notebooks and address regressions promptly.
