# Scope
This chapter is the authoritative reference for upstream systems that feed the Data BackBone. It links contractual agreements, technical connection details and operational contacts.

## Source catalogue
The table below is generated from the repository file `integration/integration_sources.csv`. Keep the csv up to date so pipelines and documentation stay aligned.

| Source | Connection type | Load enabled | Notes |
| --- | --- | --- | --- |
| Dynamics | AzureSQL | yes | Microsoft Dynamics 365 Finance & Operations (HQ) |
| Mona | SQLServer | yes | Mona legacy SQL Server for Latin America |
| Boomi | AzureSQL | yes | Dell Boomi integration platform publishing affiliate data |
| MDS | SQLServer | yes | Master Data Services hub |
| DWH | SQLServer | yes | Legacy on-premises warehouse used for backfill |
| dbb_lakehouse | AzureSQL | yes | Shortcut connections between Fabric workspaces |
| MTM | SQLServer | yes | Maritime transport management system |
| LOG | SQLServer | yes | Logistics operational database |
| STG | SQLServer | yes | Staging schema for tactical loads |
| OSS_GenericData | SQLServer | yes | Other structured sources onboarded via standard template |

Connection specific metadata (database name, workspace id, connection id) lives in `integration/integration_source_connections.csv`. Each entry must point to a managed identity or service principal with least privilege access.

## Ownership and agreements
- Business owners validate data quality and completeness when new sources go live. Capture sign-off in Azure DevOps work items.
- Technical owners maintain credentials and firewall rules. Rotation cadences should be documented per source.
- Any data privacy constraints must be reflected in Row Level Security rules and consumption layer masking.

## Operational expectations
1. Integration pipelines load metadata from `integration_source_list.csv` inside the Lakehouse to decide which tables to ingest.
2. Watermark logic stores the last successful load timestamp in `dbb_warehouse.meta.monitoring_integration`.
3. Incident response: raise a ticket, capture the failing source, run the relevant monitoring notebook and update the section below with learnings.

## Related pages
- [[Data-&-Insights-Landscape-(2.0)/31.-Source-Data-&-Agreements/Data-Source-%2D-HQ-applications-(For-Data-integrations)]]
- [[Data-&-Insights-Landscape-(2.0)/31.-Source-Data-&-Agreements/Data-Source-%2D-Affiliates-Operations-ERP-(For-Data-integrations)]]
- [[Data-&-Insights-Landscape-(2.0)/31.-Source-Data-&-Agreements/Data-source-%2D-Integration-Patterns]]

Use those subpages to capture per source agreements, refresh calendars and schema notes.
